"""
File contains functions that can be used to train LGBM (GBDT).

Author: Dominik Macko
"""

from typing import Union, Optional, Tuple

import numpy as nps
import pandas as pd
from flaml import AutoML
import optuna
import lightgbm

from .generic import train_model_flaml
from .utils import f1_weighted_loss_flaml, compute_lgbm_balanced_class_weights_arrays

def train_lgbm_flaml(train_X: Union[np.array, pd.DataFrame], 
                     train_y: Union[np.array, pd.Series], 
                     validation_X: Union[np.array, pd.DataFrame], 
                     validation_y: Union[np.array, pd.Series],
                     seconds_time_budget: int=10 * 60,
                     n_jobs=8,
                     log_file_name: str="lgbm.log"
                    ) -> AutoML:
    """Trains LGBM using FLAML library.
    
    train_X - training set features
    train_y - training set targets
    validation_X - validation set features
    validation_y - validation set targets
    metric_name - which metric to use
    seconds_time_budget - maximum time budget in seconds
    n_jobs - threads to use
    log_file_name - name of file to log
    
    returns flaml AutoML object
    """

    settings = {
        "time_budget": seconds_time_budget,
        "metric": f1_weighted_loss_flaml,
        "n_jobs": n_jobs,
        "eval_method": "holdout",
        "task": "classification",
        "log_file_name": log_file_name,
        "estimator_list": ["lgbm"]
    }
    autolgbm = train_model_flaml(
        train_X,
        train_y,
        validation_X,
        validation_y,
        settings
    )
    return autolgbm

def train_lgbm_gbdt_optuna(train_X: Union[pd.DataFrame, np.array],
                           train_y: Union[pd.Series, np.array],
                           validation_X: Union[pd.DataFrame, np.array],
                           validation_y: Union[pd.Series, np.array],
                           study_name: str="lgbm",
                           n_jobs: int=8,
                           seed: int=42,
                           early_stopping_rounds: int=3,
                           objective: str="multiclass",
                           lgbm_metric: str="softmax",
                           time_budget: int=12*60*60
                          ) -> Tuple[lightgbm.Booster, optuna.study.Study]:
    """Trains LGBM (GBDT) using Optuna
    
    train_X - training set features
    train_y - training set targets
    validation_X - validation set features
    validation_y - validation set targets
    study_name - name of Optuna study
    n_jobs - thread to use
    seed - seed to use
    early_stopping_rounds - rounds to early stop after
    objective - LGBM objective to use
    lgbm_metric - LGBM metric to use
    time_budget - maximum time budget in seconds
    
    returns (LGBM booster, Optuna study)
    """

    train_weights, validation_weights = compute_lgbm_balanced_class_weights_arrays(train_y, validation_y)
    dtrain = lightgbm.Dataset(train_X, label=train_y, weight=train_weights)
    dval = lightgbm.Dataset(validation_X, label=validation_y, weight=validation_weights)
    classes = len(np.unique(train_y))
    classes = classes if classes > 2 else 1

    params = {
        "objective": objective,
        "metric": lgbm_metric,
        "is_unbalance": True,
        "verbosity": -1,
        "boosting_type": "gbdt",
        "seed": seed,
        "objective_seed": seed,
        "num_class": classes,
        "n_jobs": n_jobs,
        "early_stopping_round": early_stopping_rounds
    }

    study = optuna.create_study(
        study_name=study_name
    )

    model = optuna.integration.lightgbm.train(
        params,
        dtrain,
        valid_sets=[dtrain, dval],
        verbose_eval=100,
        early_stopping_rounds=early_stopping_rounds,
        study=study,
        time_budget=time_budget
    )

    return (
        model,
        study
    )


