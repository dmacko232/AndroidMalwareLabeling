"""
File contains generic functions that can be used for model training.

Author: Dominik Macko
"""

from typing import Any, Callable, Dict, Tuple, List, Union, Optional

import numpy as np
import pandas as pd
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from flaml import AutoML

from .utils import build_custom_cvsplitter

def train_cv_model(model_cv: Any,
                   X: np.array,
                   y: np.array
                  ) -> Tuple[Dict[str, Any], pd.DataFrame]:
    """Trains any cross validator model.
    
    model_cv - any crossvalidator with all parameters needed, for example GridSearchCV or RandomizedSearchCV
    X - input vectors
    y - input labels
    
    returns (best_parameters, scores_df) where
        best_parameters are best hyperparameters found
        scores_df is dataframe with scores over all hyperparameter combinations
    """
    
    model_cv.fit(X, y.ravel())
    return (
        model_cv.best_params_,
        pd.DataFrame.from_dict(model_cv.cv_results_)
    )

def train_gridsearchcv_model(base_model: Any,
                             X: np.array,
                             y: np.array, 
                             cv_splitter,
                             hyperparameter_grid: Dict[str, Any],  
                             scoring: Union[str, Callable[[Any, np.array, np.array], int]]="f1_weighted",
                             n_jobs: int=4,
                             verbose: int=3,
                            ) -> Tuple[Dict[str, Any], pd.DataFrame]:
    """Trains given model using gridsearch crossvalidation.
    
    X - numpy array of input vectors
    y - numpy array of input labels
    cv - spitter that splits X and y to train and validation splits
    hyperaparameter_grid - hyperparameters used for grid search
    scoring - scoring function which is used to evaluate
    n_jobs - number of cores to use
    verbose - level of verboseness used for GridSearchCV, see scikit-learn
    
    returns (best_parameters, scores_df) where
        best_parameters are best hyperparameters found
        scores_df is dataframe with scores over all hyperparameter combinations
    """
    
    model = GridSearchCV(
        base_model, hyperparameter_grid,
        scoring=scoring,
        n_jobs=n_jobs, cv=cv_splitter, 
        refit=False, verbose=verbose,
        return_train_score=True
    )
    return train_cv_model(model, X, y)

def train_randomizedsearchcv_model(base_model: Any,
                                   X: np.array,
                                   y: np.array, 
                                   cv_splitter: Any,
                                   hyperparameter_grid: Dict[str, Any],  
                                   scoring: Union[str, Callable[[Any, np.array, np.array], int]]="f1_weighted",
                                   n_jobs: int=4,
                                   verbose: int=3,
                                   seed: int=42,
                                   n_iter: int=10
                                  ) -> Tuple[Dict[str, Any], pd.DataFrame]:
    """Trains given model using random search crossvalidation.
    
    X - numpy array of input vectors
    y - numpy array of input labels
    cv - spitter that splits X and y to train and validation splits
    hyperaparameter_grid - hyperparameters used for grid search
    scoring - scoring function which is used to evaluate
    n_jobs - number of cores to use
    verbose - level of verboseness used for GridSearchCV, see scikit-learn
    n_iter - iterations to use
    
    returns (best_parameters, scores_df) where
        best_parameters are best hyperparameters found
        scores_df is dataframe with scores over all hyperparameter combinations
    """
    
    model = RandomizedSearchCV(
        base_model, hyperparameter_grid,
        scoring=scoring,
        n_jobs=n_jobs, cv=cv_splitter, 
        refit=False, verbose=verbose,
        return_train_score=True,
        n_iter=n_iter
    )
    return train_cv_model(model, X, y)

def train_model(model: Any,
                first_train_cv: Callable,
                second_train_cv: Optional[Callable],
                grid: Dict[str, Any],
                best_params_transformer: Optional[Callable[[Dict[str, Any]], Dict[str, Any]]],
                train_X: np.array,
                train_y: np.array,
                validation_X: np.array,
                validation_y: np.array,
                scoring: Union[str, Callable[[Any, np.array, np.array], int]]="f1_weighted",
                n_jobs: int=4,
                verbose: int=3,
                seed: int=42
               ) -> Tuple[Any, pd.DataFrame]:
    """Trains any given model using two crossvalidators (the second one is optional).
    
    model - model to be trained with fit and predict methods, for example LogisticRegression
    first_train_cv - function to be used for first crossvalidation, for example train_gridsearchcv_model
    second_train_cv - optional function to be used for first crossvalidation, for example train_gridsearchcv_model
    grid - grid of parameters to use during first training
    best_params_Transformer - function that transforms grid with best results from first cv to grid for second cv
    train_X - training set features
    train_y - training set target values
    validation_X - validation set features
    validation_y - validation set features
    scoring - sklearn scoring function, for example f1_weighted
    n_jobs - cores to use, -1 means use everything
    verbose - the level of verbossnes to use
    seed - random seed to use

    returns (model, scores_df) where 
        model is given model fit to training data
        scores_df is dataframe containing scores obtained during training
    """
    
    X, y, cv = build_custom_cvsplitter(train_X, train_y, validation_X, validation_y)
    
    best_params, scores_df = first_train_cv(
        model, 
        X, y, cv, grid,
        scoring=scoring, n_jobs=n_jobs, verbose=verbose
    )
    
    if second_train_cv is not None and best_params_transformer is not None:
        best_params, new_scores_df = second_train_cv(
            model, X, y, cv, 
            best_params_transformer(best_params),
            scoring=scoring, n_jobs=n_jobs, verbose=verbose
        )
        scores_df = scores_df.append(new_scores_df)
    
    model = model.set_params(**best_params)
    return (
        model.fit(train_X, train_y.ravel()),
        scores_df
    )
    
def train_model_flaml(train_X: Union[np.array, pd.DataFrame], 
                      train_y: Union[np.array, pd.Series], 
                      validation_X: Union[np.array, pd.DataFrame], 
                      validation_y: Union[np.array, pd.Series], 
                      settings: Dict[str, Any]
                     ) -> AutoML:
    """Trains model using flaml AutoML tool.
    
    train_X - training set features
    train_y - training set targets
    validation_X - validation set features
    validation_y - validation set targets
    settings - additional settings - please refer to flaml api documentation
    
    returns AutoML model
    """

    model = AutoML()
    model.fit(
        X_train=train_X,
        y_train=train_y,
        X_val=validation_X,
        y_val=validation_y,
        **settings
        )
    return model

